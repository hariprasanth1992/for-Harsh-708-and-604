{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dab372c3",
   "metadata": {},
   "source": [
    "# PART A task 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1601831c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   variance  skewness  curtosis  entropy  class\n",
      "0   3.62160    8.6661   -2.8073 -0.44699      0\n",
      "1   4.54590    8.1674   -2.4586 -1.46210      0\n",
      "2   3.86600   -2.6383    1.9242  0.10645      0\n",
      "3   3.45660    9.5228   -4.0112 -3.59440      0\n",
      "4   0.32924   -4.4552    4.5718 -0.98880      0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\harip\\\\OneDrive\\\\Desktop\\\\Data analytics\\\\machine leaning\\\\assessment1\\\\dataset\\\\BankNote_Authentication.csv\")\n",
    "\n",
    "# Display the first few entries for a snapshot of the data structure\n",
    "print(df.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57273e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in each column:\n",
      " variance    0\n",
      "skewness    0\n",
      "curtosis    0\n",
      "entropy     0\n",
      "class       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in the dataset and print out any columns with missing data\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing values in each column:\\n\", missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f90cbc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming there are no missing values, we proceed to outlier removal\n",
    "# Calculate the Z-score for numerical columns\n",
    "z_scores = stats.zscore(df.select_dtypes(include=[np.number]))\n",
    "abs_z_scores = np.abs(z_scores)\n",
    "\n",
    "# Filter out the outliers\n",
    "df_clean = df[(abs_z_scores < 3).all(axis=1)].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "15dffc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Proceed with feature scaling on the numerical columns\n",
    "scaler = StandardScaler()\n",
    "# Get the list of numerical features, excluding 'class' which is the target\n",
    "numerical_features = [col for col in df_clean.columns if col != 'class']\n",
    "# Apply the scaling to the numerical features\n",
    "df_clean.loc[:, numerical_features] = scaler.fit_transform(df_clean[numerical_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71c55c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   variance  skewness  curtosis   entropy  class\n",
      "0  1.103540  1.186418 -1.013448  0.328655    0.0\n",
      "1  1.434725  1.097028 -0.925150 -0.179100    0.0\n",
      "2  1.191111 -0.839847  0.184664  0.605483    0.0\n",
      "3  1.044419  1.339977 -1.318300 -1.245668    0.0\n",
      "4 -0.076142 -1.165518  0.855089  0.057643    0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harip\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Encoding categorical variables, if present\n",
    "# Initialize the OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False, drop='first')\n",
    "# Identify categorical columns (Note: Adjust this if you have categorical columns)\n",
    "categorical_features = df_clean.select_dtypes(include=['object']).columns.tolist()\n",
    "# Apply the encoder to the categorical columns\n",
    "encoded_features = encoder.fit_transform(df_clean[categorical_features])\n",
    "# Create a DataFrame with encoded variables\n",
    "encoded_vars_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(categorical_features))\n",
    "# Concatenate the DataFrame with the original one and drop the original categorical columns\n",
    "df_clean = pd.concat([df_clean.drop(categorical_features, axis=1), encoded_vars_df], axis=1)\n",
    "\n",
    "# Display the cleaned DataFrame\n",
    "print(df_clean.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41aca0f4",
   "metadata": {},
   "source": [
    "# reclean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6ec65a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3329c328",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# For numerical columns, you can fill missing values with the mean or median\n",
    "imputer = SimpleImputer(strategy='mean')  # Or strategy='median'\n",
    "df_clean[numerical_features] = imputer.fit_transform(df_clean[numerical_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22879115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "df_clean[numerical_features] = imputer.fit_transform(df_clean[numerical_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7283026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All missing values handled successfully.\n"
     ]
    }
   ],
   "source": [
    "if not df_clean.isnull().sum().any():\n",
    "    print(\"All missing values handled successfully.\")\n",
    "else:\n",
    "    print(\"Missing values still present.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b858a7",
   "metadata": {},
   "source": [
    "# c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2768dfab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features Shape: (1068, 4)\n",
      "Training Target Shape: (1068,)\n",
      "Testing Features Shape: (268, 4)\n",
      "Testing Target Shape: (268,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the features and the target variable\n",
    "X = df_clean.drop('class', axis=1)  # Independent variables\n",
    "y = df_clean['class']               # Dependent variable (target)\n",
    "\n",
    "# Split the data into an 80% training subset and a 20% testing subset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Confirm the sizes of the training and testing sets\n",
    "print(f\"Training Features Shape: {X_train.shape}\")\n",
    "print(f\"Training Target Shape: {y_train.shape}\")\n",
    "print(f\"Testing Features Shape: {X_test.shape}\")\n",
    "print(f\"Testing Target Shape: {y_test.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2219c7ef",
   "metadata": {},
   "source": [
    "# task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1cbd34a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the basic model: 0.9776119402985075\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       157\n",
      "           1       0.98      0.96      0.97       111\n",
      "\n",
      "    accuracy                           0.98       268\n",
      "   macro avg       0.98      0.98      0.98       268\n",
      "weighted avg       0.98      0.98      0.98       268\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Train the classifier with the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on the testing set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Basic Model Evaluation\n",
    "print(\"Accuracy of the basic model:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "31356986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Best parameters found: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Defining the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# Initializing the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=42),\n",
    "                           param_grid=param_grid,\n",
    "                           cv=5,\n",
    "                           verbose=2,\n",
    "                           n_jobs=-1)\n",
    "\n",
    "# Performing hyperparameter tuning\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Extracting the best estimator\n",
    "rf_optimized = grid_search.best_estimator_\n",
    "\n",
    "# Displaying the best parameters\n",
    "print(\"Best parameters found:\", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c0f7af91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Random Forest Classifier Performance:\n",
      "Accuracy: 0.9776119402985075\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       157\n",
      "           1       0.98      0.96      0.97       111\n",
      "\n",
      "    accuracy                           0.98       268\n",
      "   macro avg       0.98      0.98      0.98       268\n",
      "weighted avg       0.98      0.98      0.98       268\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Making predictions with the optimized model\n",
    "y_pred_optimized = rf_optimized.predict(X_test)\n",
    "\n",
    "# Optimized Model Evaluation\n",
    "print(\"Optimized Random Forest Classifier Performance:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_optimized))\n",
    "print(classification_report(y_test, y_pred_optimized))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "91113e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[155   2]\n",
      " [  4 107]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Calculate the confusion matrix for the optimized model\n",
    "cm = confusion_matrix(y_test, y_pred_optimized)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "19aaf7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9776\n",
      "Precision: 0.9817\n",
      "Recall: 0.9640\n",
      "F1 Score: 0.9727\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Accuracy: Overall, how often is the classifier correct?\n",
    "accuracy = accuracy_score(y_test, y_pred_optimized)\n",
    "\n",
    "# Precision: When it predicts yes, how often is it correct?\n",
    "precision = precision_score(y_test, y_pred_optimized)\n",
    "\n",
    "# Recall: When it's actually yes, how often does it predict yes?\n",
    "recall = recall_score(y_test, y_pred_optimized)\n",
    "\n",
    "# F1 Score: A weighted harmonic mean of precision and recall\n",
    "f1 = f1_score(y_test, y_pred_optimized)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3ae06e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracy Scores: [0.99253731 0.99625468 0.98876404 0.99625468 0.99625468]\n",
      "Mean CV Accuracy: 0.9940130806640953\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Cross-validation for the optimized Random Forest model\n",
    "cv_scores = cross_val_score(rf_optimized, X, y, cv=5, scoring='accuracy')\n",
    "\n",
    "print(f\"Cross-Validation Accuracy Scores: {cv_scores}\")\n",
    "print(f\"Mean CV Accuracy: {cv_scores.mean()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff72070c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
